{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learning basic ML model training concepts by solving Titanic Survival Prediction problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the Problem:\n",
    "    Using the machine learning tools, we need to analyze the information about the passensgers of RMS Titanic and predict which passenger has survived. This problem has been published by Kaggle and is widely used for learning basic concepts of Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### About the data sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Data Dictionary\n",
    "\n",
    "- Age: Age\n",
    "- Cabin: Cabin\n",
    "- Embarked: Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)\n",
    "- Fare: Passenger Fare\n",
    "- Name: Name\n",
    "- Parch: Number of Parents/Children Aboard\n",
    "- Pclass: Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)\n",
    "- Sex: Sex\n",
    "- Sibsp: Number of Siblings/Spouses Aboard\n",
    "- Survived: Survival (0 = No; 1 = Yes)\n",
    "- Ticket: Ticket Number\n",
    "\n",
    "#### Variable Notes\n",
    "\n",
    "- pclass: A proxy for socio-economic status (SES)\n",
    "1st = Upper\n",
    "2nd = Middle\n",
    "3rd = Lower\n",
    "\n",
    "- age: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n",
    "\n",
    "- sibsp: The dataset defines family relations in this way...\n",
    "Sibling = brother, sister, stepbrother, stepsister\n",
    "Spouse = husband, wife (mistresses and fiancÃ©s were ignored)\n",
    "\n",
    "- parch: The dataset defines family relations in this way...\n",
    "Parent = mother, father\n",
    "Child = daughter, son, stepdaughter, stepson\n",
    "Some children travelled only with a nanny, therefore parch=0 for them.\n",
    "\n",
    "#### Download location\n",
    "training data location ->  \"https://www.kaggle.com/c/titanic/download/train.csv\" <br>\n",
    "test data location -> \"https://www.kaggle.com/c/titanic/download/test.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.0 Load data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import types\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "\n",
    "def __iter__(self): return 0\n",
    "\n",
    "# @hidden_cell\n",
    "# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n",
    "# You might want to remove those credentials before you share your notebook.\n",
    "client_366a081119f849e6862e88812b3ed98f = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id='k0nEqGfb_IxLkXnE1WOFkiQsLFZ-aVYpkGeJW-66PELy',\n",
    "    ibm_auth_endpoint=\"https://iam.ng.bluemix.net/oidc/token\",\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n",
    "\n",
    "body = client_366a081119f849e6862e88812b3ed98f.get_object(Bucket='titanicsurvivalpredictionf8684a7b97d94dde9b87f6e498cf1eb0',Key='train.csv')['Body']\n",
    "# add missing __iter__ method, so pandas accepts body as file-like object\n",
    "if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n",
    "\n",
    "training_df = pd.read_csv(body)\n",
    "training_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "body = client_366a081119f849e6862e88812b3ed98f.get_object(Bucket='titanicsurvivalpredictionf8684a7b97d94dde9b87f6e498cf1eb0',Key='test.csv')['Body']\n",
    "# add missing __iter__ method, so pandas accepts body as file-like object\n",
    "if not hasattr(body, \"__iter__\"): body.__iter__ = types.MethodType( __iter__, body )\n",
    "\n",
    "test_df = pd.read_csv(body)\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the training and test data set so that we can perform data transformations on both these sets in a single attempt. Once the data transformation is complete, the data sets have be segregated back to training and test datasets with out any mix up of samples between the data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df['Survived'] = 0\n",
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complete_data_df = training_df.append(test_df, ignore_index=True)\n",
    "complete_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"No. of Training Data samples: \" + str(training_df.shape[0]))\n",
    "print(\"No. of Test Data samples: \" + str(test_df.shape[0]))\n",
    "print(\"Complete Data samples: \" + str(complete_data_df.shape[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.0 Data Pre-processing\n",
    "\n",
    "##### 2.1 Handle Missing Data\n",
    "\n",
    "Check for missing values in the columns "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complete_data_df.isnull().sum()\n",
    "training_df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Around 80% of Cabin's data is missing. So it will not be of much use to train the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us replace the missing values for age with median. Though not a best approach to replace missing data, we shall use this method for sake of simplicity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complete_data_df['Age'] = complete_data_df['Age'].fillna(complete_data_df['Age'].median())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace missing data for Embarked. Let us use the port where maximum passengers have boarded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complete_data_df.Embarked.value_counts()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complete_data_df['Embarked'] = complete_data_df['Embarked'].fillna('S')\n",
    "complete_data_df.Embarked.unique()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2 Encode categorical feature columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the values of the categorical columns -- Sex, Embarked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def encode_features(data_set, feature_names):\n",
    "    for feature_name in feature_names:\n",
    "        le = LabelEncoder()\n",
    "        le.fit(data_set[feature_name])\n",
    "        encoded_column = le.transform(data_set[feature_name])\n",
    "        data_set[feature_name] = encoded_column\n",
    "    return data_set    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_to_encode = ['Sex', 'Embarked']\n",
    "complete_data_df = encode_features(complete_data_df, features_to_encode)\n",
    "complete_data_df.head(10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.0 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.1 Infer Title of the passengers from their names and consider it as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parsed_names = complete_data_df.Name.str.split('[,.]')\n",
    "parsed_names[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "titles = [str.strip(name[1]) for name in parsed_names.values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complete_data_df['Title'] = titles\n",
    "complete_data_df.Title.unique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Combine the titles with similar meanings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complete_data_df.Title.values[complete_data_df.Title.isin(['Mme', 'Mlle'])] = 'Mlle'\n",
    "complete_data_df.Title.values[complete_data_df.Title.isin(['Capt', 'Don', 'Major', 'Sir'])] = 'Sir'\n",
    "complete_data_df.Title.values[complete_data_df.Title.isin(['Dona', 'Lady', 'the Countess', 'Jonkheer'])] = 'Lady'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Encode the Title feature column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complete_data_df = encode_features(complete_data_df, ['Title'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complete_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 3.2 Infer if the passenger is a Minor and consider it as a feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complete_data_df['IsMinor']=np.where(complete_data_df['Age']<=16, 1, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "complete_data_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, having cleaned up the data set, let us train a model and see how it performs. But before we train the model, we need prepare the list of features that we want to use to train the model and split the combined data set back into training and test data set. As we will be doing this multiple times, let us create a function for this task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the list of features that we want to train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['Age', 'Embarked', 'Fare', 'Parch', 'Pclass', 'Sex', 'SibSp', 'Title', 'IsMinor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import cross_validation\n",
    "def get_training_data(combined_data_set):\n",
    "    training_data = combined_data_set.iloc[:891].copy()\n",
    "    return training_data\n",
    "\n",
    "def get_test_data(combined_data_set):\n",
    "    training_data = combined_data_set.iloc[892:].copy()\n",
    "    return training_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "training_data = get_training_data(complete_data_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us use the Logistic Regression algorithm for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr = LogisticRegression(random_state=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us see how good the model performs by using calculating the accuracy of the prediction on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_wo_minor = ['Age', 'Embarked', 'Fare', 'Parch', 'Pclass', 'Sex', 'SibSp', 'Title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = cross_validation.cross_val_score(lr, training_data[features_wo_minor], training_data['Survived'], cv=3)\n",
    "print(\"Score Result: \" + str(scores))\n",
    "print(\"Average Score: \" + str(scores.mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features_w_minor = ['Age', 'Embarked', 'Fare', 'Parch', 'Pclass', 'Sex', 'SibSp', 'Title', 'IsMinor']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scores = cross_validation.cross_val_score(lr, training_data[features_w_minor], training_data['Survived'], cv=3)\n",
    "print(\"Score Result: \" + str(scores))\n",
    "print(\"Average Score: \" + str(scores.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us finalize the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "selected_features = ['Age', 'Embarked', 'Fare', 'Parch', 'Pclass', 'Sex', 'SibSp', 'Title', 'IsMinor' ]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.0 Train and evaluate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "params = {'C': np.arange(1e-05, 3, 0.1)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scoring = {'Accuracy': 'accuracy', 'AUC': 'roc_auc'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs = GridSearchCV(LogisticRegression(),\n",
    "                  param_grid=params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs.fit(training_data[selected_features], training_data['Survived'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Best score: %s\" % (gs.best_score_))\n",
    "print(\"Best parameter set: %s\" % (gs.best_params_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Deploy the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a well trained model, we can deploy that in a production environment to be used the end users or applications. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, I will be using IBM Watson Machine Learning Service to deploy a trained model as a ReST service."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### IBM WML Service Credentials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "wml_credentials = {\n",
    "  \"url\": \"https://ibm-watson-ml.mybluemix.net\",\n",
    "  \"access_key\": \"xxx\",\n",
    "  \"username\": \"xxx\",\n",
    "  \"password\": \"xxx\",\n",
    "  \"instance_id\": \"xxx\"\n",
    "}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.1. Save the model to WML Repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inorder to deploy the model in WML service, the model has to be saved in the WML Repository. We will be using WML's Python client for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from repository_v3.mlrepository import MetaNames\n",
    "from repository_v3.mlrepository import MetaProps\n",
    "from repository_v3.mlrepositoryclient import MLRepositoryClient\n",
    "from repository_v3.mlrepositoryartifact import MLRepositoryArtifact\n",
    "\n",
    "import pprint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the watson_machine_learning_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ml_repository_client = MLRepositoryClient(wml_credentials['url'])\n",
    "ml_repository_client.authorize(wml_credentials['username'], wml_credentials['password'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The code below uploads the saved model's compressed tar ball in WML Repository. The API returns a bunch of metadata that was created as part of saving the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "props_meta = MetaProps({MetaNames.AUTHOR_NAME:\"Krishna\", MetaNames.AUTHOR_EMAIL:\"krishna@in.ibm.com\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_artifact = MLRepositoryArtifact(gs, name='titanic_survival_prediction', meta_props=props_meta)\n",
    "saved_model = ml_repository_client.models.save(model_artifact)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "saved_model_details = saved_model.meta.get()\n",
    "print(\"Model GUID: \" + saved_model.uid )\n",
    "pprint.pprint(saved_model_details)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5.2 Deploy the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "import urllib3\n",
    "import time\n",
    "import base64\n",
    "import requests\n",
    "import json\n",
    "import pprint\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 5.2.1 Generate token "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "headers = urllib3.util.make_headers(basic_auth='{}:{}'.format(wml_credentials['username'], wml_credentials['password']))\n",
    "url = '{}/v3/identity/token'.format(wml_credentials['url'])\n",
    "response = requests.get(url, headers=headers)\n",
    "mltoken = json.loads(response.text).get('token')\n",
    "header = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + mltoken}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get published_models url from instance details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "endpoint_instance = wml_credentials['url'] + \"/v3/wml_instances/\" + wml_credentials['instance_id']\n",
    "header = {'Content-Type': 'application/json', 'Authorization': 'Bearer ' + mltoken} \n",
    "\n",
    "response_get_instance = requests.get(endpoint_instance, headers=header)\n",
    "print(response_get_instance)\n",
    "print(response_get_instance.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "endpoint_published_models = json.loads(response_get_instance.text).get('entity').get('published_models').get('url')\n",
    "print(\"Published models url: \" + endpoint_published_models)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get deployment URL of the saved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response_models = requests.get(endpoint_published_models, headers=header)\n",
    "[deployment_url] = [x.get('entity').get('deployments').get('url') for x in json.loads(response_models.text).get('resources') if x.get('metadata').get('guid') == saved_model.uid]\n",
    "print(deployment_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare payload for deploying the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "payload_online = {\"name\": \"titanic_surv_prediction\", \"type\": \"online\"}\n",
    "response_online = requests.post(deployment_url, json=payload_online, headers=header)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit request for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print(\"Response Code: \" + str(response_online.status_code))\n",
    "pprint.pprint(response_online.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.0 Predictions based on deployed model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 6.1 Get input data for scoring from test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_data = get_test_data(complete_data_df)[selected_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_for_prediction = test_data.values[np.random.randint(test_data.shape[0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_for_prediction = input_for_prediction.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "input_for_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 6.2 Prepare JSON paylod for scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "payload_scoring = { \"values\": [input_for_prediction] }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###### 6.3 Get URL for scoring request from deployment's response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scoring_url = json.loads(response_online.text).get('entity').get('scoring_url')\n",
    "print(scoring_url)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "response_scoring = requests.post(scoring_url, json=payload_scoring, headers=header)\n",
    "pprint.pprint(response_scoring.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.0 References"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Kaggle Titanic - Machine Learning from Disaster: https://www.kaggle.com/c/titanic <br>\n",
    "IBM Data Science Experience: https://datascience.ibm.com/ <br>\n",
    "IBM Bluemix: https://console.bluemix.net/ <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
